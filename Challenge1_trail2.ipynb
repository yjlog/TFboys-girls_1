{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T22:31:15.832831500Z",
     "start_time": "2025-09-20T22:31:09.955588200Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train = pd.read_csv('data/si670_kaggle1_train.csv')\n",
    "valid = pd.read_csv('data/si670_kaggle1_validation.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a76f7185610ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T22:31:43.540996300Z",
     "start_time": "2025-09-20T22:31:15.893782800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['char_length'] = train['text'].apply(len)\n",
    "train['word_count'] = train['text'].str.split().apply(len) \n",
    "valid['char_length'] = valid['text'].apply(len)\n",
    "valid['word_count'] = valid['text'].str.split().apply(len)\n",
    "test['char_length'] = test['text'].apply(len)\n",
    "test['word_count'] = test['text'].str.split().apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e0bf266281cb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T22:32:04.845362500Z",
     "start_time": "2025-09-20T22:31:46.085833500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['avg_word_len'] = train['text'].apply(lambda x: np.mean([len(w) for w in x.split()]) if len(x.split()) > 0 else 0)\n",
    "valid['avg_word_len'] = valid['text'].apply(lambda x: np.mean([len(w) for w in x.split()]) if len(x.split()) > 0 else 0)\n",
    "test['avg_word_len'] = test['text'].apply(lambda x: np.mean([len(w) for w in x.split()]) if len(x.split()) > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4c58d9bc4d057",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T22:32:55.203940100Z",
     "start_time": "2025-09-20T22:32:40.450403800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def avg_sent_len(text):\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()] \n",
    "    if not sentences:\n",
    "        return 0\n",
    "    return np.mean([len(s.split()) for s in sentences])\n",
    "\n",
    "train['avg_sent_len'] = train['text'].apply(avg_sent_len)\n",
    "valid['avg_sent_len'] = valid['text'].apply(avg_sent_len)\n",
    "test['avg_sent_len'] = test['text'].apply(avg_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf7a34f6a3e8012",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T22:33:24.877662Z",
     "start_time": "2025-09-20T22:33:22.455480400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def punctuation_freq(text, punc):\n",
    "    return text.count(punc) / len(text) if len(text)>0 else 0\n",
    "\n",
    "train['comma_ratio'] = train['text'].apply(lambda x: punctuation_freq(x, ','))\n",
    "train['period_ratio'] = train['text'].apply(lambda x: punctuation_freq(x, '.'))\n",
    "train['question_ratio'] = train['text'].apply(lambda x: punctuation_freq(x, '?'))\n",
    "train['exclamation_ratio'] = train['text'].apply(lambda x: punctuation_freq(x, '!'))\n",
    "valid['comma_ratio'] = valid['text'].apply(lambda x: punctuation_freq(x, ','))\n",
    "valid['period_ratio'] = valid['text'].apply(lambda x: punctuation_freq(x, '.'))\n",
    "valid['question_ratio'] = valid['text'].apply(lambda x: punctuation_freq(x, '?'))\n",
    "valid['exclamation_ratio'] = valid['text'].apply(lambda x: punctuation_freq(x, '!'))\n",
    "test['comma_ratio'] = test['text'].apply(lambda x: punctuation_freq(x, ','))\n",
    "test['period_ratio'] = test['text'].apply(lambda x: punctuation_freq(x, '.'))\n",
    "test['question_ratio'] = test['text'].apply(lambda x: punctuation_freq(x, '?'))\n",
    "test['exclamation_ratio'] = test['text'].apply(lambda x: punctuation_freq(x, '!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1354798e87b20d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T22:40:41.210345900Z",
     "start_time": "2025-09-20T22:39:52.068433600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bigram_unique_ratio(text):\n",
    "    words = text.split()\n",
    "    bigrams = [tuple(words[i:i+2]) for i in range(len(words)-1)]\n",
    "    if not bigrams:\n",
    "        return 0\n",
    "    return len(set(bigrams)) / len(bigrams)\n",
    "\n",
    "def trigram_unique_ratio(text):\n",
    "    words = text.split()\n",
    "    bigrams = [tuple(words[i:i+3]) for i in range(len(words)-1)]\n",
    "    if not bigrams:\n",
    "        return 0\n",
    "    return len(set(bigrams)) / len(bigrams)\n",
    "\n",
    "train['bigram_unique_ratio'] = train['text'].apply(bigram_unique_ratio)\n",
    "train['trigram_unique_ratio'] = train['text'].apply(trigram_unique_ratio)\n",
    "valid['bigram_unique_ratio'] = valid['text'].apply(bigram_unique_ratio)\n",
    "valid['trigram_unique_ratio'] = valid['text'].apply(trigram_unique_ratio)\n",
    "test['bigram_unique_ratio'] = test['text'].apply(bigram_unique_ratio)\n",
    "test['trigram_unique_ratio'] = test['text'].apply(trigram_unique_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69bd822b0ef76c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T22:41:55.382882700Z",
     "start_time": "2025-09-20T22:41:34.608070200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def shannon_entropy(text):\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return 0\n",
    "    counts = np.array(list(Counter(words).values()))\n",
    "    probs = counts / counts.sum()\n",
    "    return -np.sum(probs * np.log2(probs))\n",
    "\n",
    "train['shannon_entropy'] = train['text'].apply(shannon_entropy)\n",
    "valid['shannon_entropy'] = valid['text'].apply(shannon_entropy)\n",
    "test['shannon_entropy'] = test['text'].apply(shannon_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b240b3b14b8d847e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T22:42:58.699146700Z",
     "start_time": "2025-09-20T22:42:58.662168100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c704a09a03f1a8fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T22:47:33.589138900Z",
     "start_time": "2025-09-20T22:47:33.521255600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_for_model = ['char_length', 'word_count', 'avg_word_len', 'avg_sent_len',\n",
    "                      'comma_ratio', 'period_ratio', 'question_ratio', 'exclamation_ratio',\n",
    "                      'bigram_unique_ratio', 'trigram_unique_ratio', 'shannon_entropy']\n",
    "X_train = train[features_for_model]\n",
    "y_train = train['label']\n",
    "X_valid = valid[features_for_model]\n",
    "y_valid = valid['label']\n",
    "X_test = test[features_for_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a290d5feda102649",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T22:48:30.720495300Z",
     "start_time": "2025-09-20T22:48:30.674943Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba0d67bf66b0d1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T22:49:40.122596600Z",
     "start_time": "2025-09-20T22:49:39.081679700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "model = LogisticRegression(penalty='l2',class_weight='balanced', max_iter=100, C=0.1, random_state=670, solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "y_valid_pred = model.predict_proba(X_valid)\n",
    "y_pred = (y_valid_pred[:, 1] >= 0.5).astype(int)\n",
    "print('F1 score:', f1_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0807f6db445b43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T22:49:53.853807100Z",
     "start_time": "2025-09-20T22:49:53.808841900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = (y_valid_pred[:, 1] >= 0.45).astype(int)\n",
    "print('F1 score:', f1_score(y_valid, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
